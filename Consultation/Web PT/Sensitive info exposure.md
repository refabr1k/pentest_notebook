
# robots.txt

robots.txt files indicate whether certain user agents (web-crawling software) can or cannot crawl parts of a website. A robots.txt file is used primarily to manage crawler traffic to your site, and usually to keep a file off Google, depending on the file type:

robots.txt reveals wsdl page

## recommendation:
What is robots.txt
https://developers.google.com/search/docs/advanced/robots/intro

WSTG-INFO-03 -
https://owasp.org/www-project-web-security-testing-guide/v41/4-Web_Application_Security_Testing/01-Information_Gathering/03-Review_Webserver_Metafiles_for_Information_Leakage

# WSDL file exposed

The Web services architecture may require exposing a Web Service Definition Language (WSDL) file that contains information on the publicly accessible services and how callers of these services should interact with them (e.g. what parameters they expect and what types they return).

Exposure of WSDL contains methods and parameters available for use


## recommendation:

Consider to not include paths that are highly targetted by attackers such as admin pages, API calls, secrets in robots.txt. 

Restrict access to WSDL file to public. If this is not possible, consider ensuring that WSDL does not describe methods that should not be publicly accessible.

## references:


CWE-651: Exposure of WSDL File Containing Sensitive Information
https://cwe.mitre.org/data/definitions/651.html 

